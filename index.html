<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Lukas Struppek </title> <meta name="author" content="Lukas Struppek"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://https//lukasstruppek.github.io//"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Lukas</span> Struppek </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?14e7bc345aa8b60c9a6ace4652dae7c7" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a research scientist at the German Research Center for Artificial Intelligence (DFKI) and a Ph.D. student at the Artificial Intelligence and Machine Learning Lab at TU Darmstadt. My research interests lie in the privacy and security of artificial intelligence (AI) and deep learning systems.</p> <p>As AI becomes more widespread and is used in critical areas such as autonomous driving, medical applications, or the financial sector, the security of the model and the privacy of training data play a crucial role. In my work, I study various attacks on machine learning models to understand and mitigate the resulting threats to safety and privacy.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 04, 2024</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2310.08320" rel="external nofollow noopener" target="_blank">“Defending Our Privacy With Backdoors”</a> got accepted at the European Conference on Artificial Intelligence (ECAI). </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 03, 2024</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2406.02366" rel="external nofollow noopener" target="_blank">“Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models”</a> got accepted at the ICML 2024 Workshop on Foundation Models in the Wild. </td> </tr> <tr> <th scope="row" style="width: 20%">May 11, 2024</th> <td> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2209.08891" rel="external nofollow noopener" target="_blank">“Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis”</a> received a Best Paper Award at the DPFM Workshop @ ICLR. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 01, 2024</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2209.07341" rel="external nofollow noopener" target="_blank">“Does CLIP Know My Face?”</a> got accepted by the Journal of Artificial Intelligence Research (JAIR). </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 04, 2024</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Two papers got accepted at ICLR Workshops: A shortened version of our paper <a href="https://www.jair.org/index.php/jair/article/view/15388/26991" rel="external nofollow noopener" target="_blank">“Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis”</a> at the <a href="https://sites.google.com/view/dpfm-iclr24/home" rel="external nofollow noopener" target="_blank">Workshop on Navigating and Addressing Data Problems for Foundation Models</a>. And our paper <a href="https://arxiv.org/pdf/2402.09132.pdf" rel="external nofollow noopener" target="_blank">“Exploring the Adversarial Capabilities of Large Language Models”</a> at the <a href="https://set-llm.github.io/" rel="external nofollow noopener" target="_blank">Workshop on Secure and Trustworthy Large Language Models</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 28, 2024</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our short paper <a href="https://arxiv.org/abs/2402.19105" rel="external nofollow noopener" target="_blank">CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI</a> got accepted at ECIS 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 01, 2024</th> <td> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> I joined the the German Research Center for Artificial Intelligence (DFKI) as a Research Scientist. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="hintersdorf24nemo" class="col-sm-8"> <div class="title">Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models</div> <div class="author"> Dominik Hintersdorf<sup>*</sup>, <em>Lukas Struppek<sup>*</sup></em>, Kristian Kersting, Adam Dziedzic, and Franziska Boenisch </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2406.02366" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ml-research/localizing_memorization_in_diffusion_models" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data, usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to the level of neurons in DMs’ cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples. By deactivating these memorization neurons, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our NeMo contributes to a more responsible deployment of DMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hintersdorf24nemo</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hintersdorf, Dominik and Struppek, Lukas and Kersting, Kristian and Dziedzic, Adam and Boenisch, Franziska}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{arXiv:2406.02366}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="struppek24smoothing" class="col-sm-8"> <div class="title">Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks</div> <div class="author"> <em>Lukas Struppek</em>, Dominik Hintersdorf, and Kristian Kersting </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2310.06549" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/LukasStruppek/Plug-and-Play-Attacks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Smoothing_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Label smoothing – using softened labels instead of hard ones – is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model’s privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against MIAs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">struppek24smoothing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Struppek, Lukas and Hintersdorf, Dominik and Kersting, Kristian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="struppek23rickrolling" class="col-sm-8"> <div class="title">Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis</div> <div class="author"> <em>Lukas Struppek</em>, Dominik Hintersdorf, and Kristian Kersting </div> <div class="periodical"> <em>In International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.02408" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/LukasStruppek/Rickrolling-the-Artist" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Rickrolling_the_Artist_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highlight that the injection process of a single backdoor takes less than two minutes. Besides phrasing our approach solely as an attack, it can also force an encoder to forget phrases related to certain concepts, such as nudity or violence, and help to make image generation safer.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">struppek23rickrolling</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Struppek, Lukas and Hintersdorf, Dominik and Kersting, Kristian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4561--4573}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="struppek23homoglyphs" class="col-sm-8"> <div class="title">Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis</div> <div class="author"> <em>Lukas Struppek</em>, Dominik Hintersdorf, Felix Friedrich, Manuel Brack, Patrick Schramowski, and Kristian Kersting </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research (JAIR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.jair.org/index.php/jair/article/view/15388/26991" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/LukasStruppek/Exploiting-Cultural-Biases-via-Homoglyphs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Models for text-to-image synthesis, such as DALL-E 2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model’s text encoder as the root cause of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we propose a novel homoglyph unlearning method to fine-tune a text encoder, making it robust against homoglyph manipulations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">struppek23homoglyphs</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Struppek, Lukas and Hintersdorf, Dominik and Friedrich, Felix and Brack, Manuel and Schramowski, Patrick and Kersting, Kristian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research (JAIR)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{78}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1017--1068}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="struppek2022ppa" class="col-sm-8"> <div class="title">Plug &amp; Play Attacks: Towards Robust and Flexible Model Inversion Attacks</div> <div class="author"> <em>Lukas Struppek</em>, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, and Kristian Kersting </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2201.12179.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/LukasStruppek/Plug-and-Play-Attacks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Plug_and_Play_Attacks_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Model inversion attacks (MIAs) aim to create synthetic images that reflect the class-wise characteristics from a target classifier’s private training data by exploiting the model’s learned knowledge. Previous research has developed generative MIAs that use generative adversarial networks (GANs) as image priors tailored to a specific target model. This makes the attacks time- and resource-consuming, inflexible, and susceptible to distributional shifts between datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks, which relax the dependency between the target model and image prior, and enable the use of a single GAN to attack a wide range of targets, requiring only minor adjustments to the attack. Moreover, we show that powerful MIAs are possible even with publicly available pre-trained GANs and under strong distributional shifts, for which previous approaches fail to produce meaningful results. Our extensive evaluation confirms the improved robustness and flexibility of Plug &amp; Play Attacks and their ability to create high-quality images revealing sensitive class characteristics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">struppek2022ppa</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{20522--20545}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Struppek, Lukas and Hintersdorf, Dominik and Correia, Antonio De Almeida and Adler, Antonia and Kersting, Kristian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Plug &amp; Play Attacks: Towards Robust and Flexible Model Inversion Attacks}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="hintersdorf2022trust" class="col-sm-8"> <div class="title"> To Trust or Not To Trust Prediction Scores for Membership Inference Attacks </div> <div class="author"> Dominik Hintersdorf<sup>*</sup>, <em>Lukas Struppek<sup>*</sup></em>, and Kristian Kersting </div> <div class="periodical"> <em>In International Joint Conference on Artificial Intelligence (IJCAI) </em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.09076.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ml-research/To-Trust-or-Not-To-Trust-Prediction-Scores-for-Membership-Inference-Attacks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/To_Trust_or_Not_To_Trust_Prediction_Scores_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Membership inference attacks (MIAs) aim to determine whether a specific sample was used to train a predictive model. Knowing this may indeed lead to a privacy breach. Most MIAs, however, make use of the model’s prediction scores - the probability of each output given some input - following the intuition that the trained model tends to behave differently on its training data. We argue that this is a fallacy for many modern deep network architectures. Consequently, MIAs will miserably fail since overconfidence leads to high false-positive rates not only on known domains but also on out-of-distribution data and implicitly acts as a defense against MIAs. Specifically, using generative adversarial networks, we are able to produce a potentially infinite number of samples falsely classified as part of the training data. In other words, the threat of MIAs is overestimated, and less information is leaked than previously assumed. Moreover, there is actually a trade-off between the overconfidence of models and their susceptibility to MIAs: the more classifiers know when they do not know, making low confidence predictions, the more they reveal the training data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hintersdorf2022trust</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3043--3049}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Artificial Intelligence ({IJCAI}) }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hintersdorf, Dominik and Struppek, Lukas and Kersting, Kristian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ To Trust or Not To Trust Prediction Scores for Membership Inference Attacks }</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="struppek2022learning" class="col-sm-8"> <div class="title"> Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash </div> <div class="author"> <em>Lukas Struppek<sup>*</sup></em>, Dominik Hintersdorf<sup>*</sup>, Daniel Neider, and Kristian Kersting </div> <div class="periodical"> <em>In ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3531146.3533073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ml-research/Learning-to-Break-Deep-Perceptual-Hashing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Learning_to_Break_Deep_Perceptual_Hashing_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system’s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">struppek2022learning</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Struppek, Lukas and Hintersdorf, Dominik and Neider, Daniel and Kersting, Kristian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash }</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Conference on Fairness, Accountability, and Transparency (FAccT)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{58--69}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=tU8K5qsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/LukasStruppek" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/lukas-struppek" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/LukasStruppek" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/306/1485.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note">You can find my e-mail address on my publications. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Lukas Struppek. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"news-tada-our-paper-learning-to-break-deep-perceptual-hashing-the-use-case-neuralhash-got-accepted-at-acm-facct-2022",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cLearning to Break Deep Perceptual Hashing: The Use Case NeuralHash\u201d...',description:"",section:"News"},{id:"news-tada-our-paper-to-trust-or-not-to-trust-prediction-scores-for-membership-inference-attacks-got-accepted-at-ijcai-ecai-2022-for-a-long-presentation-top-3-75",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cTo Trust or Not To Trust Prediction Scores for Membership...',description:"",section:"News"},{id:"news-tada-our-paper-plug-amp-amp-play-attacks-towards-robust-and-flexible-model-inversion-attacks-got-accepted-at-icml-2022",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cPlug &amp; Play Attacks: Towards Robust and Flexible Model Inversion...',description:"",section:"News"},{id:"news-trophy-our-paper-investigating-the-risks-of-client-side-scanning-for-the-use-case-neuralhash-received-the-best-paper-award-at-the-conpro-workshop-ieee-s-amp-amp-p",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Our paper \u201cInvestigating the Risks of Client-Side Scanning for the Use Case...',description:"",section:"News"},{id:"news-trophy-my-master-thesis-embedding-convolutional-mixture-of-experts-into-deep-neural-networks-for-computer-vision-tasks-received-the-faculty-award-of-the-kit-department-of-economics-and-management",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> My master thesis \u201cEmbedding Convolutional Mixture of Experts into Deep Neural Networks...',description:"",section:"News"},{id:"news-microphone-i-gave-a-talk-at-secuso-karlsruhe-institute-of-technology-with-the-titel-a-brief-history-of-security-and-privacy-in-deep-learning",title:'<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> I gave a talk at SECUSO @ Karlsruhe Institute of Technology with...',description:"",section:"News"},{id:"news-tada-our-paper-combining-ai-and-am-improving-approximate-matching-through-transformer-networks-got-accepted-at-dfrws-usa-2023",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cCombining AI and AM \u2013 Improving Approximate Matching through Transformer...',description:"",section:"News"},{id:"news-tada-our-paper-rickrolling-the-artist-injecting-backdoors-into-text-encoders-for-text-to-image-synthesis-got-accepted-at-iccv-2023",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cRickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image...',description:"",section:"News"},{id:"news-tada-our-paper-sega-instructing-diffusion-using-semantic-dimensions-got-accepted-at-neurips-2023",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cSEGA: Instructing Diffusion using Semantic Dimensions\u201d got accepted at NeurIPS...',description:"",section:"News"},{id:"news-microphone-we-gave-a-talk-at-aisola-titled-balancing-transparency-and-risk-the-security-and-privacy-risks-of-open-source-machine-learning-models",title:'<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> We gave a talk at AISoLA titled \u201cBalancing Transparency and Risk: The...',description:"",section:"News"},{id:"news-tada-our-papers-leveraging-diffusion-based-image-variations-for-robust-training-on-poisoned-data-and-defending-our-privacy-with-backdoors-got-accepted-at-the-neurips-2023-workshop-on-backdoors-in-deep-learning",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our papers \u201cLeveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data\u201d...',description:"",section:"News"},{id:"news-trophy-i-have-been-recognized-as-a-top-reviewer-at-neurips-2023",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> I have been recognized as a top reviewer at NeurIPS 2023.',description:"",section:"News"},{id:"news-tada-our-paper-exploiting-cultural-biases-via-homoglyphs-in-text-to-image-synthesis-got-published-in-the-journal-of-artificial-intelligence-research-jair",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cExploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis\u201d got published...',description:"",section:"News"},{id:"news-tada-our-paper-be-careful-what-you-smooth-for-label-smoothing-can-be-a-privacy-shield-but-also-a-catalyst-for-model-inversion-attacks-got-accepted-at-iclr-2024",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cBe Careful What You Smooth For: Label Smoothing Can Be...',description:"",section:"News"},{id:"news-loudspeaker-i-joined-the-the-german-research-center-for-artificial-intelligence-dfki-as-a-research-scientist",title:'<img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> I joined the the German Research Center for Artificial Intelligence (DFKI) as...',description:"",section:"News"},{id:"news-tada-our-short-paper-collafuse-navigating-limited-resources-and-privacy-in-collaborative-generative-ai-got-accepted-at-ecis-2024",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our short paper CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative...',description:"",section:"News"},{id:"news-tada-two-papers-got-accepted-at-iclr-workshops-a-shortened-version-of-our-paper-exploiting-cultural-biases-via-homoglyphs-in-text-to-image-synthesis-at-the-workshop-on-navigating-and-addressing-data-problems-for-foundation-models-and-our-paper-exploring-the-adversarial-capabilities-of-large-language-models-at-the-workshop-on-secure-and-trustworthy-large-language-models",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Two papers got accepted at ICLR Workshops: A shortened version of our...',description:"",section:"News"},{id:"news-tada-our-paper-does-clip-know-my-face-got-accepted-by-the-journal-of-artificial-intelligence-research-jair",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cDoes CLIP Know My Face?\u201d got accepted by the Journal...',description:"",section:"News"},{id:"news-trophy-our-paper-exploiting-cultural-biases-via-homoglyphs-in-text-to-image-synthesis-received-a-best-paper-award-at-the-dpfm-workshop-iclr",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Our paper \u201cExploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis\u201d received a...',description:"",section:"News"},{id:"news-tada-our-paper-finding-nemo-localizing-neurons-responsible-for-memorization-in-diffusion-models-got-accepted-at-the-icml-2024-workshop-on-foundation-models-in-the-wild",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cFinding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models\u201d...',description:"",section:"News"},{id:"news-tada-our-paper-defending-our-privacy-with-backdoors-got-accepted-at-the-european-conference-on-artificial-intelligence-ecai",title:'<img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper \u201cDefending Our Privacy With Backdoors\u201d got accepted at the European...',description:"",section:"News"},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=tU8K5qsAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/LukasStruppek","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/lukas-struppek","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/LukasStruppek","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/306/1485.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>